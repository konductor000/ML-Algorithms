{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1a3da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplan\\n\\n1 import \\n2 unpack data\\n21 collect paths for data\\n22 split data for train and test\\n3 create augmentation class iaa\\n4 create data generator \\n5 create vgg-16 model\\n51 optimizer, early stopping, checkpoint\\n6 fit model\\n7 predict\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plan\n",
    "\n",
    "1 import \n",
    "2 unpack data\n",
    "21 collect paths for data\n",
    "22 split data for train and test\n",
    "3 create augmentation class iaa\n",
    "4 create data generator \n",
    "5 create vgg-16 model\n",
    "51 optimizer, early stopping, checkpoint\n",
    "6 fit model\n",
    "7 predict\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D, GlobalMaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import imgaug.augmenters as iaa\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b4a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                            image  label\n",
      "0       flowers/daisy/100080576_f52e8ee070_n.jpg      0\n",
      "1       flowers/daisy/10140303196_b88d3d6cec.jpg      0\n",
      "2     flowers/daisy/10172379554_b296050f82_n.jpg      0\n",
      "3       flowers/daisy/10172567486_2748826a8b.jpg      0\n",
      "4     flowers/daisy/10172636503_21bededa75_n.jpg      0\n",
      "...                                          ...    ...\n",
      "4312   flowers/tulip/9831362123_5aac525a99_n.jpg      4\n",
      "4313   flowers/tulip/9870557734_88eb3b9e3b_n.jpg      4\n",
      "4314   flowers/tulip/9947374414_fdf1d0861c_n.jpg      4\n",
      "4315   flowers/tulip/9947385346_3a8cacea02_n.jpg      4\n",
      "4316     flowers/tulip/9976515506_d496c5e72c.jpg      4\n",
      "\n",
      "[4317 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "data_path = 'flowers'\n",
    "\n",
    "data = []\n",
    "rec = {}\n",
    "\n",
    "for idx, directory in enumerate(os.listdir(data_path)):\n",
    "    folder_path = data_path + '/' + directory\n",
    "    rec[idx] = directory\n",
    "    rec[directory] = idx\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = folder_path + '/' + file\n",
    "        data.append((file_path, idx))\n",
    "        \n",
    "data = pd.DataFrame(data, columns=['image', 'label'], index=None)\n",
    "\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5cce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_test array - (432, 224, 224, 3)\n",
      "shape of y_test array - (432, 5)\n",
      "size of X_test array - 260 MB\n"
     ]
    }
   ],
   "source": [
    "train_data, test = train_test_split(data, test_size=0.1)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "X_test, y_test = [], []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    file_path = test.image.loc[i]\n",
    "    label = test.label.loc[i]\n",
    "    image = cv2.imread(file_path)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    if image.shape[2] == 1:\n",
    "        image = np.dstack([image, image, image])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    X_test.append(image)\n",
    "    y_test.append(to_categorical(label, num_classes=5))\n",
    "    \n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "print(\"shape of X_test array -\", X_test.shape)\n",
    "print(\"shape of y_test array -\", y_test.shape)\n",
    "print(\"size of X_test array -\", sys.getsizeof(X_test) // 10 ** 6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f828568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation sequence \n",
    "seq = iaa.OneOf([\n",
    "    iaa.Fliplr(), # horizontal flips\n",
    "    iaa.Affine(rotate=20), # roatation\n",
    "    iaa.Multiply((1.2, 1.5))]) #random brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f256646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data, batch_size):\n",
    "    n = len(data)\n",
    "    steps = n // batch_size\n",
    "    \n",
    "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
    "    batch_labels = np.zeros((batch_size, 5), dtype=np.float32)\n",
    "    indices = np.arange(n)\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        counter = 0\n",
    "        np.random.shuffle(indices)\n",
    "        next_batch = indices[(batch_size*i):(batch_size*(i+1))]\n",
    "        \n",
    "        for idx in next_batch:\n",
    "            image_name = data.image.loc[idx]\n",
    "            label = data.label.loc[idx]\n",
    "            label = to_categorical(label, num_classes=5)\n",
    "            image = cv2.imread(image_name)\n",
    "            image = cv2.resize(image, (224, 224))\n",
    "            if image.shape[2] == 1:\n",
    "                image = np.dstack([image, image, image])\n",
    "            orig_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            orig_image = orig_image.astype(np.float32) / 255.0\n",
    "            \n",
    "            batch_data[counter] = orig_image\n",
    "            batch_labels[counter] = label\n",
    "            counter += 1\n",
    "                \n",
    "            if batch_size == counter:\n",
    "                break\n",
    "        \n",
    "        i += 1\n",
    "        yield batch_data, batch_labels\n",
    "        \n",
    "        if i >= steps:\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3a51d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # layer 1-2: 2 convolutional layers + 1 max-pooling layer \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = (224,224,3)))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = 2))\n",
    "\n",
    "    # number of filters and convolutions in each layer:\n",
    "    filters_convs = [(128, 2), (256, 3), (512, 3), (512,3)]\n",
    "\n",
    "    for n_filters, n_convs in filters_convs:\n",
    "        for _ in np.arange(n_convs):\n",
    "            model.add(Conv2D(filters = n_filters, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "    # max-pooling layer \n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = 2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5,activation = 'softmax'))\n",
    "\n",
    "    # compile the model with a loss function, a metric and an optimization method\n",
    "    opt = SGD(lr = 0.01) # stochastic gradient descent method with learning rate lr = 0.01\n",
    "    model.compile(loss = categorical_crossentropy, \n",
    "                optimizer = opt, \n",
    "                metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdc27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 20485     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,281,029\n",
      "Trainable params: 134,281,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "\"\"\"f = h5py.File('vgg-16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'r')\n",
    "\n",
    "w,b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1']['block1_conv1_b_1:0']\n",
    "model.layers[1].set_weights = [w,b]\n",
    "\n",
    "w,b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2']['block1_conv2_b_1:0']\n",
    "model.layers[2].set_weights = [w,b]\n",
    "\n",
    "w,b = f['block2_conv1']['block2_conv1_W_1:0'], f['block2_conv1']['block2_conv1_b_1:0']\n",
    "model.layers[4].set_weights = [w,b]\n",
    "\n",
    "w,b = f['block2_conv2']['block2_conv2_W_1:0'], f['block2_conv2']['block2_conv2_b_1:0']\n",
    "model.layers[5].set_weights = [w,b]\n",
    "\n",
    "f.close()\"\"\"\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c2938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5)\n",
    "checkpoint = ModelCheckpoint(filepath='model/best_model_todate', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ed938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training and validation steps: 242 and 432\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "nb_epochs = 20\n",
    "\n",
    "train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
    "\n",
    "nb_train_steps = train_data.shape[0]//batch_size\n",
    "\n",
    "print(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7adf96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "201/242 [=======================>......] - ETA: 51s - loss: 1.6019 - accuracy: 0.2484"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
    "                               validation_data=(X_test, y_test),callbacks=[es, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8b243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5b369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5672e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
