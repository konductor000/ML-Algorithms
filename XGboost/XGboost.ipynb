{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "63612312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "38539b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostRegression:\n",
    "    \n",
    "    def __init__(self, params={}, random_seed=None):\n",
    "        \n",
    "        self.params = defaultdict(lambda: None, params)\n",
    "        self.verbose = self.params['verbose'] or False\n",
    "        self.n_estimators = self.params['n_estimators'] or 50\n",
    "        self.subsample = self.params['subsample'] or 1.0\n",
    "        self.learning_rate = self.params['learning_rate'] or 0.3\n",
    "        self.base_prediction = params['base_score'] or 0.5\n",
    "        self.max_depth = self.params['max_depth'] or 5\n",
    "        self.rng = np.random.default_rng(seed=random_seed)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "        self.boosters = []\n",
    "        objective = self.Objective()\n",
    "        for i in range(self.n_estimators):\n",
    "            gradients = objective.gradient(y, current_predictions)\n",
    "            hessian = objective.hessian(y, current_predictions)\n",
    "            sample_idxs = None if self.subsample == 1.0 \\\n",
    "                else self.rng.choice(len(y),\n",
    "                    size=math.floor(len(y)*self.subsample),\n",
    "                    replace=False)\n",
    "            booster = TreeBooster(X, gradients, hessian, self.params,\n",
    "                        self.max_depth, sample_idxs)\n",
    "            current_predictions = self.learning_rate * booster.predict(X)\n",
    "            self.boosters.append(booster)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.base_prediction + self.learning_rate\n",
    "            * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n",
    "        \n",
    "    class Objective:\n",
    "        def loss(self, y, pred): \n",
    "            return np.mean((y - pred)**2)\n",
    "        def gradient(self, y, pred): \n",
    "            return pred - y\n",
    "        def hessian(self, y, pred): \n",
    "            return np.ones(len(y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5dc5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeBooster:\n",
    "    \n",
    "    def __init__(self, X, gradients, hessian, params, max_depth, sample_idxs=None):\n",
    "        self.params = params\n",
    "        self.max_depth = max_depth\n",
    "        assert max_depth >= 0, 'max_depth must be nonnegative'\n",
    "        self.min_child_weight = self.params['min_child_weight'] or 1.0\n",
    "        self.reg_lambda = self.params['reg_lambda'] or 1.0\n",
    "        self.gamma = self.params['gamma'] or 0.0\n",
    "        self.colsample_bynode = self.params['colsample_bynode'] or 1.0\n",
    "        if isinstance(gradients, pd.Series): \n",
    "            gradients = gradients.values\n",
    "        if isinstance(hessian, pd.Series): \n",
    "            hessian = hessian.values\n",
    "        if sample_idxs is None:\n",
    "            sample_idxs = np.arange(len(gradients))\n",
    "        self.X, self.gradients, self.hessian, self.sample_idxs = X, gradients, hessian, sample_idxs\n",
    "        self.n, self.c = len(self.sample_idxs), X.shape[1]\n",
    "        self.value = -gradients[sample_idxs].sum() / \\\n",
    "            (hessian[sample_idxs].sum() + self.reg_lambda)\n",
    "        self.current_best_score = 0.0\n",
    "        if self.max_depth > 0:\n",
    "            self.maybe_insert_child_nodes()\n",
    "                \n",
    "    def maybe_insert_child_nodes(self):\n",
    "        for i in range(self.c):\n",
    "            self.find_best_split(i)\n",
    "        if self.is_leaf:\n",
    "            return\n",
    "        x = self.X.values[self.sample_idxs, self.split_feature_idx]\n",
    "        left_idx = np.nonzero(x <= self.threshold)[0]\n",
    "        right_idx = np.nonzero(x > self.threshold)[0]\n",
    "        self.left = TreeBooster(self.X, self.gradients, self.hessian,\n",
    "            self.params, self.max_depth - 1, self.sample_idxs[left_idx])\n",
    "        self.right = TreeBooster(self.X, self.gradients, self.hessian, \n",
    "            self.params, self.max_depth - 1, self.sample_idxs[right_idx])\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.current_best_score == 0.0\n",
    "\n",
    "    def find_best_split(self, feature_idx):\n",
    "        x = self.X.values[self.sample_idxs, feature_idx]\n",
    "        gradients, hessian = self.gradients[self.sample_idxs], self.hessian[self.sample_idxs]\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_gradients, sort_hessian, sort_x = gradients[sort_idx], hessian[sort_idx], x[sort_idx]\n",
    "        sum_gradients, sum_hessian = gradients.sum(), hessian.sum()\n",
    "        sum_gradients_right, sum_hessian_right = sum_gradients, sum_hessian\n",
    "        sum_gradients_left, sum_hessian_left = 0.0, 0.0\n",
    "\n",
    "        for i in range(self.n - 1):\n",
    "            gradients_i, hessian_i, x_i, x_i_next = sort_gradients[i], \\\n",
    "            sort_hessian[i], sort_x[i], sort_x[i+1]\n",
    "            sum_gradients_left += gradients_i\n",
    "            sum_gradients_right -= gradients_i\n",
    "            sum_hessian_left += hessian_i\n",
    "            sum_hessian_right -= hessian_i\n",
    "            if sum_hessian_left < self.min_child_weight or x_i == x_i_next:continue\n",
    "            if sum_hessian_right < self.min_child_weight: break\n",
    "\n",
    "            gain = 0.5 * ((sum_gradients_left**2 / (sum_hessian_left + self.reg_lambda))\n",
    "                            + (sum_gradients_right**2 / (sum_hessian_right + self.reg_lambda))\n",
    "                            - (sum_gradients**2 / (sum_hessian + self.reg_lambda))\n",
    "                            ) - self.gamma/2 # Eq(7) in the xgboost paper\n",
    "            if gain > self.current_best_score: \n",
    "                self.split_feature_idx = feature_idx\n",
    "                self.current_best_score = gain\n",
    "                self.threshold = (x_i + x_i_next) / 2\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n",
    "\n",
    "    def _predict_row(self, row):\n",
    "        if self.is_leaf: \n",
    "            return self.value\n",
    "        child = self.left if row[self.split_feature_idx] <= self.threshold \\\n",
    "            else self.right\n",
    "        return child._predict_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e744dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "488e2792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:43:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'reg_lambda': 1.5,\n",
    "    'gamma': 0.0,\n",
    "    'min_child_weight': 25,\n",
    "    'base_score': 0.0,\n",
    "    'tree_method': 'exact',\n",
    "}\n",
    "\n",
    "num_boost_round = 50\n",
    "# train the from-scratch XGBoost model\n",
    "model_scratch = XGBoostRegression(params, random_seed=42)\n",
    "model_scratch.fit(X_train, y_train)\n",
    "\n",
    "# train the library XGBoost model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "model_xgb = xgb.train(params, dtrain, num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8cab58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredErrorObjective():\n",
    "    def loss(self, y, pred): return np.mean((y - pred)**2)\n",
    "    def gradient(self, y, pred): return pred - y\n",
    "    def hessian(self, y, pred): return np.ones(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "329a83d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch score: 70.9466429657219\n",
      "xgboost score: 0.24123239765807963\n"
     ]
    }
   ],
   "source": [
    "pred_scratch = model_scratch.predict(X_test)\n",
    "pred_xgb = model_xgb.predict(dtest)\n",
    "print(f'scratch score: {SquaredErrorObjective().loss(y_test, pred_scratch)}')\n",
    "print(f'xgboost score: {SquaredErrorObjective().loss(y_test, pred_xgb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04188ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59666d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed4cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
